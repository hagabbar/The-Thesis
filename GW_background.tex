\chapter{Gravitational waves}

\section{Gravitational Waves}

Gravitational waves were predicted by Einstein in his theory of general relativity well over 100 years ago. In his theory, Einstein formalises the relationship between matter and space-time as

\begin{equation}
    G_{\mu \nu} + \Lambda g_{\mu \nu} = \frac{8 \pi G}{c^{4}} T_{\mu \nu},
\end{equation}{}

where $\Lambda$ is the cosmological constant (scalar measurement describing the energy density of space), $g_{\mu \nu}$ is the metric which describes the geometric structure of space-time, $G$ is Newton's gravitational constant, $c$ is the speed of light, $T_{\mu \nu}$ is the stress energy tensor which describes the density, direction, flow of energy in space-time and $G_{\mu \nu}$ is the Einstein tensor defined as

\begin{equation}
    G_{\mu \nu} = R_{\mu \nu} - \frac{1}{2} R g_{\mu \nu}
\end{equation}

where $R_{\mu \nu}$ is the Rici curvature tensor which determines the degree to which matter will tend to change as a function of time and $R$ is a real valued scalar number describing the degree of curvature. 

Gravitational waves may be defined as small perturbations over the curved background spacetime metric $\bar{g}_{\mu \nu}$

\begin{equation}
    g_{\mu \nu} = \bar{g}_{\mu \nu}(x) + h_{\mu \nu}(x),
\end{equation}{}

where $h_{\mu \nu}(x)$ is the gravitational wave perturbation. This perturbation is both plus and cross polarized, denoted as

\begin{equation}
    h_{\plus} = 
    h_{\cross} = 
\end{equation}{}

\section{The Detectors}

The \ac{LIGO} gravitational wave detectors are composed of 
two detectors, one in Hanford, Washington State and the 
other in Livingston, Louisiana. There are also other ground-based detectors 
in Hannover, Germany (GEO), Pisa, Italy (Virgo) and Kamioka, Japan 
(KAGRA). In addition to ground-based detectors there are eventual plans 
to build a space-based observatory called the \ac{LISA} which 
will search for super massive binary black holes (among other 
sources). Each detector can be thought 
of as a large-scale Michelson-Morley Interferometer composed 
of two arms orthogonal to each other. Each arm of 
the detectors is 4km in length. The strain a \ac{GW} induces on two free point masses 
can be expressed as 

\begin{equation}
    h(t) = \frac{2 \Delta L}{L},
\end{equation}

where $h(t)$ is the strain amplitude of the \ac{GW} 
as a function of time, $\Delta L$ is 
the relative change in length induced by the \ac{GW} on the two 
point masses and $L$ is the baseline length between the point 
masses without a \ac{GW} present. 

We record the change in length 
on the two point masses through the use of a ND:Yag 20W 
laser and large mirrors. Photons emited from the laser pass through a beam 
splitter and down the two arms of the detectors. The 
photons then hit end test mass mirrors at both ends 
and are then caught in a Fabry-Perot signal recylcing 
cavity in order to effectively increase the baseline 
length of the arms. Eventually, some photons are released 
from the Fabry-Perot cavities and return to the beam splitter 
and are recorded on a set of photodiodes which measure 
the phase difference between photons from both 
arms.

Through some algebraic manipulation we can get an estimate on the 
light travel time difference between the two interferometer 
arms as 

\begin{equation}
    \Delta \tau(t) = h(t) \frac{2L}{c} = h(t) \tau_{rt0}.
\end{equation}

%
% Need to figure out what tau rt0 is ...
%
where $\tau_{rt0}$ is the return trip time down 
one arm and the phase difference being 

\begin{equation}
    \Delta \phi(t) = h(t) \tau_{rt0} \frac{2\pi c}{\lambda}.
\end{equation}

Here we can clearly see that the phase difference 
between the two light signals is scaled by the 
length of the interferometer arms $L$. Because the 
detectors are tuned such that the photons arriving 
back at the beam splitter act to destructively 
interfere with each other, there should theoretically 
be no signal hitting the photodiodes in nominal operating times. When a \ac{GW} 
impinges on the detector, it will compress one arm 
while stretching the other arm. There will then be a 
noticeable difference in phase between the light traveling 
down both arms. Because of this difference in phase, the 
light recombining at the beam splitter will no longer 
destructively interfere and a signal will appear 
on the photodetectors.

\subsection{Detector noise}

Although this change in phase between 
the two detector arms can be measured through 
photon counting, the detectors are 
also sensitive to non-astrophysical noise 
sources. These noise sources can drastically affect 
the sensitivity of the detectors and may also mimic 
gravitational wave events. Some common noise sources 
include anthropgenic noise, quantum shot noise, 
seismic noise and thermal noise. 

Seismic noise largely affects the sensitivity of 
the detectors in the low frequency regime where 
an earthquake produces sets of waves which travel both 
through the earths core/mantel and also along the 
surface of the earth. When one of these seismic waves 
hits the detectors it can knock the interferometer arms 
out of alignment. Closely related to seismic noise 
(though at different frequencies) anthropogenic noise 
can result from individuals walking around in the 
\ac{LIGO} control rooms or large trucks passing 
on a nearby highway.

%
% Not sure if this is right. 
%
Thermal noise results from the heat produced by 
the lasers passing through large coated mirrors in 
the interferometer. When laser photons pass through 
these mirrors, the photons subsequently heat up 
the coating/mirror material. This heat causes the molecules 
which make up the coating/lens material to behave in 
random Brownian motion, thus causing the photons to deviate from 
their intended path.

Quantum shot noise results from the wave packet-like 
behavior of light as it travels through a medium. 
Because of Poisson statistics, we know that the 
uncertainty on the number of photons that we 
count arriving at the \ac{LIGO} photodiodes 
after recombination at the beam splitter is 
proportional to the expected number of photons 
arriving each second. The greater the power 
of the laser, the greater the quantum shot 
noise at higher photon frequencies.

%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%% 
\section{LIGO sources}

There are a variety of events which can cause significant 
enough distortions in spacetime to produce 
\ac{GW} events. Such events include \ac{CBC} signals, burst events,
continuous \ac{GW}s from rotating neutron stars and 
stochastic \ac{GW}s (leftover echoes from the Big Bang). In this 
section I will describe these events and how \ac{GW}s are 
produced from them.

\subsection{Compact Binary Coalescencs}

\ac{CBC} signals arise from the collision of massive compact 
objects moving at high relativistic speeds. Such systems can 
include binary black holes, neutron star - black hole pairs, binary 
neutron stars and super massive black hole encounter events. 

As the two objects rotate about each other, energy is radiated away in the 
form of \ac{GW}s due to the asymmetric motion of the heavy objects. 
Over the course of millions of years, the two objects will 
inspiral in towards each other. In so doing the objects will 
orbit faster, peturbing spacetime to a greater degree and 
thus releasing more energy in the form \ac{GW}s. In the final few 
seconds prior to merger the objects will release a large amount of 
\ac{GW} energy collide. If the objects are binary black holes, they 
will coalesce into a single black hole which will ``ring'' for a 
short amount of time. If the objects are binary neutron stars they will 
typically collide and then produce a large supernovoe event, emiting 
a large amount of \ac{EM} light in the process (which importantly 
can be measured by other telescopes across the spectrum on Earth). 

\subsection{Continuous Waves}

\ac{CW} signals are canonically associated with spinning non-axisymmetric 
neutron stars. Other more exotic sources can result from clouds of bosons 
annihilating each around fast-spinning black holes. 
Neutron stars are the leftover cores of dead stars 
which have exploded in a supernovae and then collapsed down 
into an object roughly the mass of our sun and with a radius of a 
few km. \ac{GW} signals are produced from mass quadrupole radiation in the 
gravitational field and spinning neutron stars can cause such 
disturbances through cracking and cooling of the crust, internal non-axisymmetric 
magnetic field flows, and mountains of mass accreted on the 
surface from a larger companion star \cite{1712.05897}.

\subsection{Stochastic Gravitational Waves}

\subsection{Burst signals}

\section{Search methods for gravitational wave signals}

In this section, I will describe several methods used by the 
\ac{LVC} to search for signals from \ac{CBC}, \ac{GW}, burst and 
stochastic \ac{GW}s.

\subsection{\ac{CW} search method}

There are estimated to be roughly $\sim 10^{8} - 10^{9}$ neutrons stars 
in our own Milky Way Galaxy, of which only $\sim 2500$ have already 
been observed by the wider scientific community. One method for detecting 
\ac{CW} signals is to go after these already known $2500$ neutron 
stars and perform a targeted search.

\subsubsection{Targeted search}
The GW strain given by a non-axisymmetric spinning neutron star 
is typically defined by 

\begin{equation}
    h_{0} = \frac{4\pi^2G}{c^4} \frac{I_{zz} f^2}{d} \epsilon,
\end{equation}

where $I_{zz}$ is the moment of intertia around the z-axis of the 
neutron star, $f$ is the \ac{GW} frequency (roughly proportional to 
the star spin frequency) and $\epsilon$ is the elipticity. In a targetted 
search, we generally assume that the sky position, frequency, spin and 
other parameters are well known. There is also a middle-ground option of 
performing a directed search where we only assume to know the sky location 
very well. Nominally, a \ac{FFT} is first performed on the 
data afterwhich a transformation of some sort is then applied (
frequency-Hough, ``stack-slide'', PowerFlux) in order to 
map the observed data to source properties. After this mapping performed, 
we essentially then try to look for excesses in power above 
a pre-defined threshold over a large number of frequency bins. The 
methods listed above put many different spins on this\cite{PhysRevD.90.042002}.

\subsubsection{All-sky search}

\subsection{Burst search method}

\subsection{Stochastic search method}

\subsection{\ac{CBC} search method}

The output of the \ac{LIGO} gravitational wave detector is a time series produced by the resulting phase shift of two lasers after recombination on a photodiode. Because our detector is not perfectly isolated from all non-astrophysical sources, the output of the detector will be a function of both a gravitational wave impinging on it, as well some noise

\begin{equation}
    s(t) = h(t) + n(t),
\end{equation}{}

where $h(t)$ is the combined strain from a gravitational wave induced on the interferometer and $n(t)$ is the noise contribution. We generally assume that the noise is both stationary and Gaussian, although in reality the detector noise can be non-Gaussian. For those cases where we have non-Gaussianity we run various glitch identification tools and techniques to identify problematic areas of the detector data output. 

Assuming Gaussian noise, our problem then becomes, how does one distinguish noise from actual signal? Fortunately, the problem of extracting low \ac{SNR} signals from the background is not uncommon in physics and the field of statistics and may be accomplished through a technique known as matched template filtering.

\subsection{Matched template filtering}

Given the current level of sensitivity of the detectors, we will be dealing within a regime where the \ac{GW} signal we are trying to detect will often be burried far below the overall background of the detector noise. We can take advantage of the fact that we generally have a good understanding of the form of $h(t)$. If we know exactly the form of $h(t)$, we may construct a simple filtering technique whereby we multiply the output of the detector $s(t)$ by $h(t)$ and integrate over some observation time. The noise contribution term of the expression

\begin{equation}
    \frac{1}{T} \int_0^T dt n(t) h(t) \sim (\frac{\tau_{0}}{T})^{(1/2)} n_{0} h_{0}
\end{equation}{}

as we increase the observation time $T$, we see that the overall value of the function tends to zero. So, given enough observation time, we can effectively filter out the contribution from the noise. The keen reader will spot one issue. Given that we may not have an infinite amount of observation time due to the duty cycle of the detectors and the fact that amplitude of the gravitational wave signal is not constant as a function of time, how can we make this filtering technique more optimal given finite observation time $T$. We define an optimal signal-to-noise ratio

\begin{equation}
    SNR_{opt} = 4 \int_0^{\infty} df \frac{|\bar{h}(f)|^2}{S_{n}(f)},
\end{equation}{}

where $|\bar{h}(f)|^2$ is is the inner product of the ideal signal template with itself and $S_{n}(f)$ is the single-sided noise \ac{PSD}. In matched template filtering, we generate many thousands of templates and compute the optimal \ac{SNR} of each. The best matching will contain the highest optimal \ac{SNR} for that event. If the \ac{SNR} is above the detection threshold (usually defined as 8), we say that there is a candidate gravitational wave signal present in the data.

\subsection{Bayesian gravitational wave parameter estimation}

%
% Use this reference: https://www.cambridge.org/core/journals/publications-of-the-astronomical-society-of-australia/article/an-introduction-to-bayesian-inference-in-gravitationalwave-astronomy-parameter-estimation-model-selection-and-hierarchical-models/D459F61D8C37F0BEF86D60F42A418304
%

%
% Introduce Bayes theorem
%
It is not only important that we detect a \ac{GW} event, but also that we infer the underlying properties of that event in the form of its source parameters (i.e. component mass, distance, sky location, etc.). In \ac{LIGO}, the tried and true method for inferring source parameters is done through Bayes theorem. Bayes theorem was first used by Reverand Thomas Bayes in the 18th century and in it he proposed a new paradigm for thinking about the laws of conditional probability. To describe succinctly, Bayes theorem says that one can infer the limits on an unknown parameter by computing the likelihood of a given observation and our prior belief on the distribution of the unknown parameter. To put it in the context of \ac{GW} astronomy, given an observed gravitational wave event and some prior assumptions, we would like to infer the source parameter values of that signal while also taking into account the uncertainty added by the signal being burried in noise. We call the inferred source parameters with uncertainty the posterior 

%
% Introduce posterior
%
\begin{equation}
    p(\theta | d),
\end{equation}

where $p(\theta | d)$ is the probability of the source parameters of the signal ($\theta$ being a continuous variable), given some observed data (in the form of a time/frequency series). We assume that the integral over the total posterior is normalised such that 

%
% State that posterior is normalised such it integrates to 1
%
\begin{equation}
    \int d\theta p(\theta | d) = 1.
\end{equation}

According to Bayes theorem, we can write the posterior as 

%
% Show Bayes theorem
%
\begin{equation}
    p(\theta | d) = \frac{p(d|\theta)p(\theta)}{p(d)},
\end{equation}

%
% Discussion on priors
%
where $p(d|\theta)$ is the likelihood of our observable $d$ given source parameters $\theta$, $p(\theta$) is our prior assumptions on the distribution of the source parameters $\theta$ and $p(d)$ is a normalisation factor called the evidence. The prior $p(\theta)$ is largely informed by our understanding on the formation channels of \ac{GW} sources and our level of understanding on the general physics which govern events. For example, we would intuitively think that the mass of an object should always be positive, so will set the priors such that the component masses of a \ac{GW} source must always lie between two positive values. However, if we aren't as knowledgeable about a particular parameter $\theta$, we might try choosing a relatively uninformative prior by employing something like a broad uniform distribution. Choice of prior can also be incredibly influential on the conclusions drawn on some \ac{GW} parameters such as spin. For further interesting discussions on prior choices see Salvatore et al. \cite{PhysRevLett.119.251103}.

%
% Discussion on likelihood
%
The way in which we define the likelihood $p(d|\theta)$ (our certainty on the observed data) is essentially up to the practitioner. For \ac{GW} astronomy, we define a likelihood which assumes that the detectors operate under Gaussian noise-like conditions. The Gaussian-noise likelihood function can written as 

\begin{equation}
    p(d|\theta) = \frac{1}{\sqrt{2\pi \sigma^2}} \textrm{exp}\left(-\frac{1}{2} 
    \frac{(d - \mu)^2}{\sigma^2}\right),
\end{equation}

where $\sigma$ is the detector noise, $d$ is the observed data and $\mu$ is a template \ac{GW} waveform parameterized by source parameters $\theta$. 

%
% Discussion on the evidence
%
The evidence $p(d)$ can be defined as 

\begin{equation}
    p(d) = \int p(d|\theta) p(\theta) d\theta.
    \label{eq:evidence}
\end{equation}

The evidence is usually referred to as the marginal likelihood. Because we are marginalising over all parameters $\theta$ in Eq. \ref{eq:evidence}, we can think of the evidence as essentially a normalising factor. Since the evidence is just simply a normalising factor and it is prohibitively expensive to compute this factor, most Bayesian practitioners will ignore Eq. \ref{eq:evidence} and rewrite Bayes theorem in the simpler form 

\begin{equation}
    p(\theta | d) \propto p(d | \theta) p(\theta).
\end{equation}

\subsubsection{Markov Chain Monte Carlo}

%
% Intro and Monte Carlo sampling
% Great video on this: https://www.youtube.com/watch?v=OTO1DygELpY&ab_channel=StataCorpLLC
\ac{MCMC} is usually used when we would like to try and sample from some distribution $p$, or approximate the expectation value of some function $f(x)$ which is of a high dimension/complexity. Where $p$ is so complicated that trying to sample from $p$ analytically would be prohibitively expensive. The Monte Carlo portion of \ac{MCMC} refers to a technique known as Monte Carlo sampling. Monte Carlo sampling means to randomly sample from some distribution. For example, I could choose to randomly sample from a normal distribution $N(0,1)$ or from uniform distribution between -1 and 1. We would define the normal or the uniform distribution that I'm sampling from the proposal distribution. We should see if we randomly sample from the proposal distribution enough times, that a histogram of the resulting samples should resemble that of the original proposal distribution. 

%
% Markov Chain
%
Markov Chain refers to how we are going to go about sampling from that space. Specifically, a Markov Chain is a sequence of numbers where each number in the sequence is dependent on the previous number in the sequence. For example, if we again decided to randomly sample from proposal distribution $N(0,1)$, but instead after each sample is drawn we change the mean of the proposal distribution to be equal to that of the previous sample $N(x_{n-1},1)$, we would end up with something known as a random walk. I will note hear that this approach would not necessarily reproduce the original proposal distribution.

%
% How to accept/reject proposals
%

We can use a variety of algorithms to go about deciding how exactly accept or reject new samples drawn from the proposal distribution. One common algorithm for doing so is the Metropolis-Hastings algorithm. This algorithms works by first calculating the likelihood value for the new proposed sample, as well as the likelihood value of the previously proposed sample, and then computes the ratio of these two values. If the likelihood value of the new proposed sample is greater than the old value, then we will always accept the new value. If however, the new value is not greater than the old value, then we will not necessarily discard the old value. Instead, we can treat the new proposed likelihood value as an acceptance probability. In order to determine acceptance in this case, we can draw a uniform random number between 0 and 1 and keep the new value if it's likelihood is greater than the randomly drawn number.

%
% Issues with Metropolis Hastings
%
Unfortunately there are a few downsides to using the Metropolis Hastings algorithm. One them being that we have to choose a starting point for the random walk, which is initially liable to be far from the true posterior. Thus it may take some iterations for the algorithm to walk its way towards areas of high likelihood. We can avoid this issue by discarding an arbitrary number of samples at the beginning of the walk such that the remaining represent a point after which the algorithm has reached a stable equilibrium. We call the discarded samples the burn-in period. Another issue relates to something known as autocorrelation. Predictions on parameters $\theta$ are known to be somewhat correlated with each other due to the fact that they are all generated from the same Markov process. The fact that this correlation exists is fine, but excessive correlation can mean that there are issues with the model being used. A helpful method for tempering such correlations can be through the process of thinning. Thinning involves generating a large amount of samples from the proposal distribution, but only keeping every $N^{\textrm{th}}$ sample from that large sample set. 

%
% Could be useful to make a trace plot here.
%

\subsubsection{Nested Sampling}

Nested sampling is another method used in order to explicitly sample from the posterior. This is done by first drawing $N$ random points from the prior distribution $p(\theta)$. We then iterate over a pre-determined number of iterations. For each iteration we compute the likelihood of all sampled points and determine the point with the minimum likelihood value. We then compute an estimate for the amount of prior mass covered by the the current samples above the minimum likelihood value sample.

\begin{equation}
    Z = \int p(\theta) L(\theta) d\theta
\end{equation}

where $p(\theta)$ is the prior distribution and $L(\theta)$ is the likelihood function.

\begin{equation}
    X(L^{*}) = \int p(\theta) \mathds{1} (L(\theta) > L^{*}) d\theta
\end{equation}

where $X(L^{*})$ represents the amount of prior probability with likelihood greater than $L^{*}$.
%
% Intro to nested sampling
%

\subsubsection{Nuisance parameter marginalization}